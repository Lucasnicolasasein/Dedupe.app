{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# Importing libraries and necessary functions\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_dedupe\n",
    "import os\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "------------------------------------------------------------------------------------------------------------------------\n",
    "DeDuping Tool\n",
    "------------------------------------------------------------------------------------------------------------------------\n",
    "Date: January 2022\n",
    "Considerations:\n",
    "    -\n",
    "Authors:\n",
    "        Clara Graham                        clara.graham@accenture.com\n",
    "        Lucas Asein                         lucas.asein@accenture.com\n",
    "        Shameez Manji                       shameez.manji@accenture.com\n",
    "        Ignacio Brottier González           ignacio.brottier@accenture.com\n",
    "        Arielle Jaraba Serrano              arielle.serrano@accenture.com\n",
    "\"\"\"\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "# Importing libraries and necessary functions\n",
    "# ----------------------------------------------------------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_dedupe\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "#  ----------------------------------------------------------------------------------------------------------------------\n",
    "# Model functions\n",
    "# # ----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def dupe_train(df_sample):\n",
    "    # 1. Delete old training file\n",
    "    ans = yes_no_answer('Would you like to retrain the model?')\n",
    "    if ans:\n",
    "        # Try to delete the file ##\n",
    "        try:\n",
    "            os.remove('dedupe_dataframe_learned_settings')\n",
    "            print('\\tSettings file removed')\n",
    "        except OSError as e:  ## if failed, report it back to the user ##\n",
    "            print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "        try:\n",
    "            os.remove('dedupe_dataframe_training.json')\n",
    "            print('\\tJson file removed')\n",
    "        except OSError as e:  ## if failed, report it back to the user ##\n",
    "            print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "\n",
    "    print(f'\\n\\tSample Shape: {df_sample.shape}')\n",
    "    print('\\tSample Head\\n')\n",
    "    print(df_sample.head())\n",
    "\n",
    "    # 2. Select columns for training\n",
    "    columns_list = [select_col('Enter the column indexes to use when training the model', df_sample, False)]\n",
    "    print(f'Selected Columns: {columns_list}')\n",
    "    not_done = yes_no_answer('Do you want to add more columns?')\n",
    "    while not_done:\n",
    "        columns_list.append(select_col('Enter additional columns to train the model', df_sample, False))\n",
    "        columns_list = list(set(columns_list))\n",
    "        print(f'Selected Columns: {columns_list}')\n",
    "        if len(columns_list) < len(list(df_sample.columns)):\n",
    "            not_done = yes_no_answer('Do you want to add more columns?')\n",
    "        else:\n",
    "            not_done = True\n",
    "    print(f'\\nColumns for clustering will be {columns_list}\\n')\n",
    "\n",
    "    # 3. Clean Data\n",
    "    for c in range(0, len(columns_list)):\n",
    "        df_sample[columns_list[c]] = df_sample[columns_list[c]].astype('str')\n",
    "        df_sample[columns_list[c]] = df_sample[columns_list[c]].str.strip()\n",
    "        df_sample[columns_list[c]] = df_sample[columns_list[c]].str.replace('\\n', ' ')\n",
    "\n",
    "    # 4. Run Model\n",
    "    print('Training clustering model')\n",
    "    df_train = pandas_dedupe.dedupe_dataframe(df_sample, columns_list)\n",
    "\n",
    "    # Sort values by cluster_id\n",
    "    df_train.sort_values('cluster id', inplace=True)\n",
    "    df_train.reset_index(inplace=True)\n",
    "    df_train.drop(columns=['index'], inplace=True)\n",
    "\n",
    "    return df_train\n",
    "\n",
    "\n",
    "#  ----------------------------------------------------------------------------------------------------------------------\n",
    "# UI/UX functions\n",
    "# # ----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def load_file(msg):\n",
    "    found = False\n",
    "    while not found:\n",
    "        file_path = input(msg)\n",
    "        try:\n",
    "            found = True\n",
    "            df = pd.read_csv(file_path)\n",
    "        except FileNotFoundError:\n",
    "            print('File not found. Please try again\\n')\n",
    "            found = False\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    print(f'\\n\\tThe file {file_path} was loaded correctly\\n')\n",
    "    print(df.head())\n",
    "    print('\\n')\n",
    "    return df, cols\n",
    "\n",
    "\n",
    "def select_col(msg, df, print_df=True):\n",
    "    print(f'{msg}\\n')\n",
    "    if print_df:\n",
    "        print(df.tail())\n",
    "        print()\n",
    "\n",
    "    cols = list(df.columns)\n",
    "    options = {i: cols[i] for i in range(len(cols))}\n",
    "    print(options)\n",
    "    value = input(f'\\n')\n",
    "    while int(value) not in options.keys():\n",
    "        print(f'Invalid option - {msg}\\n')\n",
    "        print(options)\n",
    "        value = input(f'\\n')\n",
    "\n",
    "    ans_ = cols[int(value)]\n",
    "    print(f'\\t Columns Selected: {ans_}')\n",
    "    if not yes_no_answer('Continue?'):\n",
    "        select_col(msg, df, False)\n",
    "\n",
    "    return ans_\n",
    "\n",
    "\n",
    "def yes_no_answer(msg):\n",
    "    msg = msg + ' (y) Yes - (n) No\\n'\n",
    "    value = input(msg)\n",
    "    value = str(value).lower()\n",
    "    while value not in ['y', 'n']:\n",
    "        print(value)\n",
    "        print('Invalid Answer. Please try again')\n",
    "        value = input(msg)\n",
    "        value = value.lower()\n",
    "\n",
    "    ans_ = True if value == 'y' else False\n",
    "    return ans_\n",
    "\n",
    "\n",
    "def preview_results(df, n=5):\n",
    "    ans_ = yes_no_answer('Do you want to preview some results?')\n",
    "\n",
    "    # Sort values by confidence\n",
    "    df.sort_values('confidence', inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    df.drop(columns=['index'], inplace=True)\n",
    "\n",
    "    x = 0\n",
    "    cluster_id_list = list(df['cluster id'].unique())\n",
    "    while ans_:\n",
    "        for i in range(0, n):\n",
    "            cluster = cluster_id_list[x]\n",
    "            df_example = df[df['cluster id'] == cluster]\n",
    "\n",
    "            print(f'\\tCluster id: {cluster}')\n",
    "            print(f'\\tCluster confidence: {df_example[\"confidence\"].mean()}\\n')\n",
    "\n",
    "            df_example = df_example.sample(n=min(len(df_example), 5))\n",
    "            print(df_example)\n",
    "\n",
    "            x += 1\n",
    "\n",
    "        ans_ = yes_no_answer('Do you want to preview more results?')\n",
    "\n",
    "\n",
    "def break_line():\n",
    "    aux = 100 * \"=\"\n",
    "    print(f'\\n\\t\\t{aux}\\n')\n",
    "\n",
    "\n",
    "#  ----------------------------------------------------------------------------------------------------------------------\n",
    "# Main\n",
    "# # ----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('\\t\\t\\t\\tWelcome to DeDupping Tool\\n\\n')\n",
    "\n",
    "    # 1. Load Data\n",
    "    #       Excel files closed warning\n",
    "    print('All related spreadsheets should be closed')\n",
    "    #       Load New Data File\n",
    "    df_data, list_cols = load_file('Please enter the name of the spreadsheet you would like to dedupe. Make sure to add the “.csv” extension.\\n')\n",
    "\n",
    "    break_line()\n",
    "\n",
    "    # 2. Merge with previous deduped file\n",
    "    has_base_data = yes_no_answer('Would you like to append the output of this data to an existing deduped file?')\n",
    "    if has_base_data:\n",
    "        # Load Normalized Names File\n",
    "        df_norm_names, list_norm_cols = load_file('Please enter the name of the existing deduped csv file. Make sure to include the “.csv” extension.\\n')\n",
    "        # Select Columns\n",
    "        #       Select Name Column New Data\n",
    "        column_name = select_col('Select which column from the new data has the client name', df_data)\n",
    "        #       Select Unique ID Base Data\n",
    "        column_norm_id = select_col('Select which column from the base data has the client unique id', df_norm_names)\n",
    "        #       Select Name Column Base Data\n",
    "        column_norm_name = select_col('Select which column from the base data has the client name', df_norm_names, False)\n",
    "\n",
    "        # Maintain original clusters\n",
    "        #   Append one entry of each old cluster id - name to maintain nomenclature during new clusterization\n",
    "        print('\\nMaintaining original Cluster Id Nomenclature')\n",
    "        print(f'{len(df_norm_names[column_norm_id].unique())} Unique Cluster Ids found...')\n",
    "        new_row = {col: \"\" for col in list_cols}\n",
    "        for id_aux in tqdm(df_norm_names[column_norm_id].unique()):\n",
    "            cluster_name = df_norm_names[df_norm_names[column_norm_id] == id_aux].sample(n=1)\n",
    "            cluster_name = cluster_name.iat[0, list_norm_cols.index(column_norm_name)]\n",
    "            new_row[column_name] = cluster_name\n",
    "            df_data = df_data.append(new_row, ignore_index=True)\n",
    "        print('\\tMerging successful')\n",
    "\n",
    "    break_line()\n",
    "\n",
    "    ans = False\n",
    "    while not ans:\n",
    "        # 3. Train Dedupping model\n",
    "        df_final = dupe_train(df_data)\n",
    "        print('\\tDedupping model trained successfully\\n\\n')\n",
    "\n",
    "        # 4. Preview Results\n",
    "        preview_results(df_final)\n",
    "        ans = yes_no_answer('Are the examples OK, or should we retrain?')\n",
    "        if not ans:\n",
    "            del df_final\n",
    "\n",
    "    # 5. Merge with original data\n",
    "    if has_base_data:\n",
    "        print('Merging with previous deduped data...')\n",
    "        df_norm_names[column_norm_name] = df_norm_names[column_norm_name].str.upper()\n",
    "        df_final[column_name] = df_final[column_name].str.upper()\n",
    "        aux_old_id = max(df_norm_names[column_norm_id])\n",
    "\n",
    "        df_join = pd.merge(\n",
    "            df_norm_names,\n",
    "            df_final,\n",
    "            how='outer',\n",
    "            left_on=[column_norm_name],\n",
    "            right_on=[column_name]\n",
    "        )\n",
    "\n",
    "        id_mapping = {}\n",
    "        for id_aux in tqdm(df_final['cluster id'].unique()):\n",
    "            max_old = max(df_join.loc[df_join['cluster id'] == id_aux][column_norm_id])\n",
    "            if max_old > 0:\n",
    "                id_mapping[id_aux] = max_old\n",
    "            else:\n",
    "                id_mapping[id_aux] = aux_old_id\n",
    "                aux_old_id += 1\n",
    "        df_aux = df_final.copy()\n",
    "        df_aux['cluster id'] = df_aux['cluster id'].map(id_mapping)\n",
    "        df_aux = df_aux.rename(columns={column_name: column_norm_name, 'cluster id': column_norm_id})\n",
    "        df_aux = df_aux[[column_norm_name, column_norm_id]]\n",
    "\n",
    "        df_save = pd.concat([df_aux,df_norm_names]).drop_duplicates().reset_index(drop=True)\n",
    "        #df_save = pd.concat([df_aux, df_norm_names]).reset_index(drop=True)\n",
    "\n",
    "        for id_aux in df_norm_names[column_norm_id].unique():\n",
    "            assert len(df_norm_names[df_norm_names[column_norm_id] == id_aux]) <= len(df_save.loc[df_save[column_norm_id] == id_aux])\n",
    "\n",
    "    else:\n",
    "        df_final = df_final[column_name, 'cluster id']\n",
    "        df_save = df_final.rename(columns={column_name: 'Original Supplier Name', 'cluster id': 'Normalized Name Unique ID'})\n",
    "\n",
    "    # 6. Save Results\n",
    "    df_save.to_csv('normalized_names_new.csv', index=False)\n",
    "\n",
    "    print(\"\\n\\nThat would be all, thank you for using the dedupping tool.\")\n",
    "    break_line()\n",
    "    print('\\n\\n'\n",
    "          '\\tJanuary 2022\\n'\n",
    "          '\\tIgnacio Brottier González           ignacio.brottier@accenture.com\\n'\n",
    "          '\\tClara Graham                        clara.graham@accenture.com\\n'\n",
    "          '\\tLucas Asein                         lucas.asein@accenture.com\\n'\n",
    "          '\\tShameez Manji                       shameez.manji@accenture.com\\n'\n",
    "          '\\tArielle Jaraba Serrano              arielle.serrano@accenture.com')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
